"use strict";(self.webpackChunkunmanic_documentation=self.webpackChunkunmanic_documentation||[]).push([[967],{2068:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"guides/unmanic_kubernetes_install","title":"Install Unmanic - Kubernetes","description":"Unmanic Kubernetes Install Guide","source":"@site/docs/guides/unmanic_Kubernetes_Guide.md","sourceDirName":"guides","slug":"/guides/unmanic_kubernetes_install","permalink":"/docs/guides/unmanic_kubernetes_install","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Install Unmanic - Kubernetes","description":"Unmanic Kubernetes Install Guide","id":"unmanic_kubernetes_install"},"sidebar":"docs","previous":{"title":"Install Unmanic - MacOS","permalink":"/docs/guides/unmanic_macos_install"},"next":{"title":"Linking Unmanic Installations","permalink":"/docs/guides/unmanic_link_installations"}}');var s=i(4848),r=i(8453);const o={title:"Install Unmanic - Kubernetes",description:"Unmanic Kubernetes Install Guide",id:"unmanic_kubernetes_install"},a=void 0,l={},d=[{value:"Requirements",id:"requirements",level:2},{value:"Running Unmanic",id:"running-unmanic",level:2},{value:"Running Unmanic Rootless",id:"running-unmanic-rootless",level:2},{value:"Tuning Kubernetes Unmanic Configuration",id:"tuning-kubernetes-unmanic-configuration",level:2},{value:"Accessing Unmanic via NodePort",id:"accessing-unmanic-via-nodeport",level:2},{value:"Linux Hardware Acceleration (VA-API)",id:"linux-hardware-acceleration-va-api",level:2},{value:"Intel Integrated GPU",id:"intel-integrated-gpu",level:3},{value:"Prerequistes",id:"prerequistes",level:3},{value:"Validate Hardware Compatibility",id:"validate-hardware-compatibility",level:3},{value:"Install Intel GPU drivers",id:"install-intel-gpu-drivers",level:3},{value:"Install and Configure Intel GPU Plugin",id:"install-and-configure-intel-gpu-plugin",level:3},{value:"Install and Configure Node Feature Discovery (NFD)",id:"install-and-configure-node-feature-discovery-nfd",level:3},{value:"Manifest Configuration Considerations",id:"manifest-configuration-considerations",level:3},{value:"Resource Management",id:"resource-management",level:4},{value:"Supplemental Device Permissions",id:"supplemental-device-permissions",level:4},{value:"Affinity Rules",id:"affinity-rules",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:"https://img.shields.io/badge/Difficulty-moderate-orange?style=flat",alt:"Difficulty"}),"\n",(0,s.jsx)(n.img,{src:"https://img.shields.io/badge/Setup%20Time-30%20minutes-orange?style=flat",alt:"Setup Time"}),"\n",(0,s.jsx)(n.a,{href:"https://github.com/preimmortal",children:(0,s.jsx)(n.img,{src:"https://img.shields.io/badge/Original%20Author-preimmortal-lightgrey?style=flat?style=plastic&logo=github",alt:"Original Author"})})]}),"\n",(0,s.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsx)(n.p,{children:"This requires a functional Kubernetes Cluster or similar (k3s, minikube, etc)."}),"\n",(0,s.jsx)(n.p,{children:"For Kubernetes installation instructions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/setup/",children:"Kubernetes Getting Started Guide"})}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"K3S 1.22.7 was used to create this guide."})}),"\n",(0,s.jsx)(n.h2,{id:"running-unmanic",children:"Running Unmanic"}),"\n",(0,s.jsx)(n.p,{children:"There are two parts to running Unmanic in Kubernetes."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Deployment - The unmanic container configuration"}),"\n",(0,s.jsx)(n.li,{children:"Service - The service that exposes the unmanic container to outside the Kubernetes cluster"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For a basic deployment and service, create a file ",(0,s.jsx)(n.code,{children:"unmanic.yaml"})," and append the following:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: unmanic-deployment\nspec:\n  selector:\n    matchLabels:\n      app: unmanic\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: unmanic\n    spec:\n      containers:\n        - name: unmanic\n          image: josh5/unmanic:latest\n          ports:\n            - containerPort: 8888\n              name: unmanic-port\n              protocol: TCP\n          env:\n            - name: PUID\n              value: "<PUID>"\n            - name: PGID\n              value: "<PGID>"\n          volumeMounts:\n            - mountPath: /library\n              name: library\n            - mountPath: /config\n              name: unmanic-config\n            - mountPath: /tmp/unmanic\n              name: unmanic-cache\n      volumes:\n        - name: library\n          nfs:\n            server: <NFS_SERVER_ADDRESS>\n            path: </PATH/TO/NFS/SHARE>\n        - name: unmanic-config\n          emptyDir: {} # Please use a more permanent storage, see Tuning section\n        - name: unmanic-cache\n          emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: unmanic-service\nspec:\n  selector:\n    app: unmanic\n  type: NodePort\n  ports:\n    - name: unmanic-port\n      port: 8888\n      targetPort: 8888\n      nodePort: 30000\n'})}),"\n",(0,s.jsxs)(n.p,{children:["To start the deployment and service, run the following command: ",(0,s.jsx)(n.code,{children:"kubectl create -f unmanic.yaml"})]}),"\n",(0,s.jsxs)(n.p,{children:["To delete the deployment and service, run the following command: ",(0,s.jsx)(n.code,{children:"kubectl delete -f unmanic.yaml"})]}),"\n",(0,s.jsx)(n.h2,{id:"running-unmanic-rootless",children:"Running Unmanic Rootless"}),"\n",(0,s.jsx)(n.p,{children:"In kubernetes you can run containers as non root, doing so will allow you to harden the system for outside/inside threats.\nUse securityContext to set the user to be used by the app, make sure your library storage is also set to the same uid/gid as you specify here."}),"\n",(0,s.jsxs)(n.p,{children:["To not run the s6-overlay(which needs root) add ",(0,s.jsx)(n.code,{children:'command: ["/usr/local/bin/unmanic-service"]'}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Specify as a env the home location which is supposed to be set to the config volume location."}),"\n",(0,s.jsx)(n.p,{children:"Use only as reference"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: unmanic\n  labels:\n    app: unmanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: unmanic\n  template:\n    metadata:\n      labels:\n        app: unmanic\n    spec:\n      securityContext:\n        runAsUser: 568 # in this case i will be using user 568, change how you like\n        runAsGroup: 568\n        fsGroup: 568\n      containers:\n      - name: unmanic\n        image: josh5/unmanic:latest\n        command: ["/usr/local/bin/unmanic-service"] # Hard requirement for running rootless\n        ports:\n        - containerPort: 8888\n          protocol: TCP\n          name: http\n        env:\n        - name: HOME # Hard requirement for running rootless\n          value: "/config" \n        volumeMounts:\n        - name: unmanic-config\n          mountPath: /config\n        - name: media\n          mountPath: /media\n      volumes:\n        - name: unmanic-config\n          persistentVolumeClaim:\n            claimName: unmanic-rbd # In this example I am using a persistentvolume claim, change as you see fit\n        - name: media\n          nfs:\n            server: IP\n            path: /path/to/export/folder\n'})}),"\n",(0,s.jsx)(n.p,{children:"Its possible that not all plugins will work.\nTested with: Transcode Video Files and File Size Metrics Data Panel"}),"\n",(0,s.jsx)(n.h2,{id:"tuning-kubernetes-unmanic-configuration",children:"Tuning Kubernetes Unmanic Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Note that the following values should be tuned based on need:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Using NFS to share the library path to Kubernetes, please update as necessary if using CIFS or another file sharing service"}),"\n",(0,s.jsx)(n.li,{children:"Using emptyDir for the config and cache path, please update emptyDir to more permanent paths"}),"\n",(0,s.jsx)(n.li,{children:"Using NodePort to expose the Unmanic service, please update when using a load balancer or other method to expose the unmanic deployment"}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{type:"note",children:[(0,s.jsxs)(n.p,{children:["It is important not to use ",(0,s.jsx)(n.code,{children:"emptyDir"})," for the ",(0,s.jsx)(n.code,{children:"unmanic-config"})," in a permanent installation because the config will be deleted upon stopping the deployment.\nPlease consider using a more permanent volume such as iSCSI or backing up to NFS."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/storage/volumes/",children:"Kubernetes Volumes"})}),"\n"]})]}),"\n",(0,s.jsx)(n.h2,{id:"accessing-unmanic-via-nodeport",children:"Accessing Unmanic via NodePort"}),"\n",(0,s.jsxs)(n.p,{children:["Unmanic will be available via the web browser at following address: ",(0,s.jsx)(n.code,{children:"<Kubernetes_Cluster_Address>:30000"})]}),"\n",(0,s.jsx)(n.h2,{id:"linux-hardware-acceleration-va-api",children:"Linux Hardware Acceleration (VA-API)"}),"\n",(0,s.jsx)(n.h3,{id:"intel-integrated-gpu",children:"Intel Integrated GPU"}),"\n",(0,s.jsx)(n.p,{children:"It is possible to expose underlying Intel integrated GPU hardware that supports QSV/VA-API hardware acceleration into a Kubernetes workload. The proper method for implementing this functionality is through two additional Kubernetes add-ons:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://kubernetes-sigs.github.io/node-feature-discovery/",children:"Node feature discovery"})," to automatically detect and label worker nodes with specialized hardware."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/intel/intel-device-plugins-for-kubernetes",children:"Intel GPU Plugin"})," to automatically expose GPU hardware to workloads, e.g. ",(0,s.jsx)(n.code,{children:"/dev/dri/renderD128"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"This guide does not walk through the full installation and configuration for the aforementioned Kubernetes add-ons. This process is relatively straightforward using the most up-to-date guides from the associated links to upstream repositories."})}),"\n",(0,s.jsx)(n.h3,{id:"prerequistes",children:"Prerequistes"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Validate hardware compatibility"}),"\n",(0,s.jsx)(n.li,{children:"Install Intel GPU drivers"}),"\n",(0,s.jsxs)(n.li,{children:["Install and configure ",(0,s.jsx)(n.a,{href:"https://github.com/intel/intel-device-plugins-for-kubernetes",children:"intel-gpu-plugin"})," add-on"]}),"\n",(0,s.jsxs)(n.li,{children:["Install and configure ",(0,s.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/node-feature-discovery",children:"node-feature-discovery"})," add-on"]}),"\n",(0,s.jsx)(n.li,{children:"Kubernetes manifest considerations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"validate-hardware-compatibility",children:"Validate Hardware Compatibility"}),"\n",(0,s.jsxs)(n.p,{children:["The Kubernetes worker node exposing GPU functionality to workloads requires Intel GPU drivers to be installed at the host level. An official lookup table exists for ",(0,s.jsx)(n.a,{href:"https://dgpu-docs.intel.com/devices/hardware-table.html",children:"GPU hardware values"})," to confirm they are supported. In the following example, ",(0,s.jsx)(n.code,{children:"[8086:9a49]"})," indicates the vendor ",(0,s.jsx)(n.code,{children:"8086"})," (Intel) and ",(0,s.jsx)(n.code,{children:"9a49"})," (GPU; Iris Xe) from the lookup table."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"$ lspci -nn |grep  -Ei 'VGA|DISPLAY'\n00:02.0 VGA compatible controller [0300]: Intel Corporation TigerLake-LP GT2 [Iris Xe Graphics] [8086:9a49] (rev 01)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"install-intel-gpu-drivers",children:"Install Intel GPU drivers"}),"\n",(0,s.jsxs)(n.p,{children:["The official Intel documentation outlines instructions for installation of the appropriate ",(0,s.jsx)(n.a,{href:"https://dgpu-docs.intel.com/installation-guides/ubuntu/ubuntu-jammy-legacy.html",children:"Intel drivers on Ubuntu 22.04 (jammy)"}),". Once the drivers have been installed, a reboot is required before validation steps below:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'# Install hwinfo and vainfo\n$ apt update && apt install -y hwinfo vainfo\n\n# Confirm Intel GPU drivers are functional\n$ hwinfo --display\n25: PCI 02.0: 0300 VGA compatible controller (VGA)\n  [Created at pci.386]\n  Unique ID: _Znp.vvQ429ch_ZF\n  SysFS ID: /devices/pci0000:00/0000:00:02.0\n  SysFS BusID: 0000:00:02.0\n  Hardware Class: graphics card\n  Device Name: "Onboard - Video"\n  Model: "Intel UHD Graphics"\n  Vendor: pci 0x8086 "Intel Corporation"\n  Device: pci 0x9a49 "UHD Graphics"\n  SubVendor: pci 0x8086 "Intel Corporation"\n  SubDevice: pci 0x3002\n  Revision: 0x01\n  Driver: "i915"\n  Driver Modules: "i915"\n  Memory Range: 0x603c000000-0x603cffffff (rw,non-prefetchable)\n  Memory Range: 0x4000000000-0x400fffffff (ro,non-prefetchable)\n  I/O Ports: 0x3000-0x303f (rw)\n  Memory Range: 0x000c0000-0x000dffff (rw,non-prefetchable,disabled)\n  IRQ: 177 (15621845 events)\n  Module Alias: "pci:v00008086d00009A49sv00008086sd00003002bc03sc00i00"\n  Driver Info #0:\n    Driver Status: i915 is active\n    Driver Activation Cmd: "modprobe i915"\n  Config Status: cfg=new, avail=yes, need=no, active=unknown\n\nPrimary display adapter: #25\n\n# Verify profiles supported by hardware\n$ vainfo\nlibva info: VA-API version 1.16.0\nlibva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\nlibva info: Found init function __vaDriverInit_1_16\nlibva info: va_openDriver() returns 0\nvainfo: VA-API version: 1.16 (libva 2.12.0)\nvainfo: Driver version: Intel iHD driver for Intel(R) Gen Graphics - 22.6.4 (aca8ee0)\nvainfo: Supported profile and entrypoints\n... truncated list of profiles ...\n'})}),"\n",(0,s.jsx)(n.h3,{id:"install-and-configure-intel-gpu-plugin",children:"Install and Configure Intel GPU Plugin"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"https://github.com/intel/intel-device-plugins-for-kubernetes",children:"intel-gpu-plugin"})," add-on will provide the ability to expose ",(0,s.jsx)(n.code,{children:"i915"})," devices to pods within the cluster. Due to the continually changing nature of Kubernetes, it's recommended to use the official documentation for installation of this add-on. This process will effectively passthrough the ",(0,s.jsx)(n.code,{children:"/dev/dri/*"})," devices to workloads that are requesting it."]}),"\n",(0,s.jsxs)(n.p,{children:["By default, passthrough will consume the associated device for that workload. It is possible to share a device with multiple workloads at the same time. Runtime argument ",(0,s.jsx)(n.code,{children:"-shared-dev-num 2"})," allows two workloads access to the GPU simultaneously. Additional information or runtime options can be found ",(0,s.jsx)(n.a,{href:"https://intel.github.io/intel-device-plugins-for-kubernetes/cmd/gpu_plugin/README.html#modes-and-configuration-options",children:"here"}),"."]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["It's important to note that exposing the GPU device is only one part of the equation. You also need to ensure any pods consuming the resources have adequate permissions to access the device. This can be achieved with addition of ",(0,s.jsx)(n.code,{children:"supplementalGroups: [44, 109]"})," in the associated workload manifest. This is documented in the Supplemental Device Permissions section."]})}),"\n",(0,s.jsx)(n.h3,{id:"install-and-configure-node-feature-discovery-nfd",children:"Install and Configure Node Feature Discovery (NFD)"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/node-feature-discovery",children:"node-feature-discovery"})," add-on is used to automatically label worker nodes with specialized hardware. Due to the continually changing nature of Kubernetes, it's recommended to use the official documentation for installation. Once deployed within a Kubernetes cluster, nodes containing Intel GPU hardware will be labeled with ",(0,s.jsx)(n.code,{children:"feature.node.kubernetes.io/custom-intel-gpu=true"}),". You can use this label to apply affinity rules that will only schedule workloads on nodes with appropriate hardware."]}),"\n",(0,s.jsx)(n.h3,{id:"manifest-configuration-considerations",children:"Manifest Configuration Considerations"}),"\n",(0,s.jsx)(n.p,{children:"Additional configuration within Kubernetes is required to take advantage of the underlying hardware. The following sections will provide additional detail depending on your needs."}),"\n",(0,s.jsx)(n.h4,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsxs)(n.p,{children:["Providing resource management constraints will help to ensure a workload has adequate resources. Kubernetes will schedule workloads where the resources are available, allowing them to consume more than the ",(0,s.jsx)(n.code,{children:"requests"})," values, but not beyond the ",(0,s.jsx)(n.code,{children:"limit"})," value. In the following example, the workload is allowed access to the GPU device exposed through the Intel GPU plugin. Additionally, a maximum of 4 CPU cores and 8GB of memory can be consumed by the workload. The following YAML should be applied in the ",(0,s.jsx)(n.code,{children:"deployment.spec.template.spec"})," section from the deployment manifest example."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"      resources:\n        requests:\n          cpu: 100m\n          memory: 256M\n          gpu.intel.com/i915: 1\n        limits:\n          cpu: 4\n          memory: 8G\n          gpu.intel.com/i915: 1\n"})}),"\n",(0,s.jsx)(n.h4,{id:"supplemental-device-permissions",children:"Supplemental Device Permissions"}),"\n",(0,s.jsxs)(n.p,{children:["Devices exposed by Intel GPU drivers in ",(0,s.jsx)(n.code,{children:"/dev/dri/*"})," have special group permissions for ",(0,s.jsx)(n.code,{children:"video"})," and ",(0,s.jsx)(n.code,{children:"render"})," operating system groups."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'$ ls -la /dev/dri\ntotal 0\ndrwxr-xr-x  3 root root        100 Jan 21 23:15 .\ndrwxr-xr-x 20 root root       4840 Jan 23 09:15 ..\ndrwxr-xr-x  2 root root         80 Jan 21 23:15 by-path\ncrw-rw----  1 root video  226,   0 Jan 21 23:15 card0\ncrw-rw----  1 root render 226, 128 Jan 21 23:15 renderD128\n\n$ egrep "video|render" /etc/group\nvideo:x:44:\nrender:x:109:\n'})}),"\n",(0,s.jsxs)(n.p,{children:["In order to utilize the devices in a workload, you must ensure supplemental group permissions are applied. This can be achieved by adding a ",(0,s.jsx)(n.code,{children:"securityContext"})," to the above deployment manifest. The following YAML should be applied in the ",(0,s.jsx)(n.code,{children:"deployment.spec.template.spec"})," section from the deployment manifest example."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"      securityContext:\n        supplementalGroups:\n          - 44\n          - 109\n"})}),"\n",(0,s.jsx)(n.h4,{id:"affinity-rules",children:"Affinity Rules"}),"\n",(0,s.jsxs)(n.p,{children:["Affinity rules allow or disallow workloads from being placed where they shouldn't. For example, if only two worker nodes have compatible Intel GPU hardware, using the NFD node label ",(0,s.jsx)(n.code,{children:"feature.node.kubernetes.io/custom-intel-gpu"})," will ensure workloads requiring GPU access to be scheduled on the corresponding nodes. The following YAML should be applied in the ",(0,s.jsx)(n.code,{children:"deployment.spec.template.spec"})," section from the deployment manifest example."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: feature.node.kubernetes.io/custom-intel-gpu\n                    operator: In\n                    values:\n                      - "true"\n'})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);